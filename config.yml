general:
  timezone: "UTC"
  log_level: "INFO"
  base_path: "/data"

exchanges:
  - name: "binance"
    symbols:
      # Core Crypto Set
      - SOLUSDT
      - SUIUSDT
      - ADAUSDT
      - BTCUSDT
      - ETHUSDT
      - BNBUSDT
      - XRPUSDT
      - DOGEUSDT
      - AVAXUSDT
      - LINKUSDT
      - LTCUSDT
      - DOTUSDT
      # FX Majors
      - EURUSDT
    rest_url: "https://api.binance.com/api/v3"
    wss_url: "wss://stream.binance.com:9443/ws"

collector:
  write_interval_sec: 60
  reconnect_backoff: 10
  output_format: "jsonl"

transformer:
  resample_interval_sec: 1
  parquet_compression: "snappy"
  schedule_minutes: 60  # Run transformer every 60 minutes in orchestrate mode (0 to disable)

macro_minute:
  # List of macro/FX tickers to fetch (1-minute data from yfinance)
  tickers:
    - SPY          # S&P 500 ETF
    - UUP          # US Dollar Index Fund
    - ES=F         # E-mini S&P 500 Futures
    - EURUSD=X     # EUR/USD
    - GBPUSD=X     # GBP/USD
    - USDJPY=X     # USD/JPY
    - USDCAD=X     # USD/CAD
    - USDCHF=X     # USD/CHF
  # Schedule parameters for orchestrate mode
  schedule_minutes: 15            # Fetch macro data every 15 minutes
  startup_backfill_days: 7        # One-time backfill on startup (max 7 days for yfinance 1m data)
  runtime_lookback_days: 1        # Fetch last 1 day on each scheduled run

qa:
  # QA Sidecar Configuration
  enable_ai: true                 # Enable AI/statistical anomaly detection
  hourly_window_min: 30           # Window for hourly QA runs (minutes) - reduced for testing
  daily_run_utc: "00:15"          # Daily QA run time (UTC, HH:MM format)
  ai_labeler: "rules"             # Labeler type: "rules", "llm", or "hybrid"

  # IsolationForest detector configuration
  iforest:
    n_estimators: 200             # Number of trees
    contamination: 0.005          # Expected anomaly proportion (0.5%)
    random_state: 42              # Random seed for reproducibility

  # Z-score detector configuration
  zscore:
    window: 3600                  # use 3600 instead of "1h"
    k: 5.0                        # Z-score threshold

  # Jump detector configuration
  jump:
    k_sigma: 6.8                  # Jump threshold in standard deviations
    spread_stable_bps: 50         # Maximum spread in bps for "stable" condition
    cooldown_seconds: 5           # Cluster window: merge anomalies within this window
    min_trade_count: 5            # Skip ultra-thin seconds with fewer trades

  # Reporting configuration
  reporting:
    anomalies_top_n: 10           # Show top-N symbols by anomaly count in reports

testing:
  # Testing mode for rapid validation (minutes instead of hours)
  enabled: false                  # Can be overridden by CLI flags --mode test or --testing
  transform_interval_min: 2       # Transform every 2 minutes (vs 60 in production)
  macro_interval_min: 1           # Fetch macro data every 1 minute (vs 15 in production)
  macro_transform_interval_min: 1 # Transform macro data every 1 minute in test mode
  macro_lookback_startup_days: 1  # Backfill 1 day on startup (vs 7 in production)
  macro_runtime_lookback_days: 1  # Fetch 1 day on each run (same as production)
  backfill_days: 3                # Backfill 3 days for testing (vs 180 in production)
  base_path: "D:/CryptoDataLake/test"  # Isolated test directory

# Optional: SQL database configuration
# If not specified, falls back to DuckDB in-memory for QA operations
# database:
#   # Option 1: Full connection string with embedded credentials
#   url: "duckdb:///./crypto.duckdb"
#   # Examples:
#   #   url: "sqlite:///crypto_lake.db"                    # Local SQLite file
#   #   url: "postgresql://user:pass@localhost:5432/crypto_lake"  # PostgreSQL
#   #   url: "postgresql://user:pass@/crypto_lake?host=/cloudsql/project:region:instance"  # Google Cloud SQL
#   #   url: "postgresql+pg8000://user:pass@/crypto_lake?unix_sock=/cloudsql/project:region:instance/.s.PGSQL.5432"  # GCP Cloud SQL with pg8000
#
#   # Option 2: Separate credential fields (used as fallback if url doesn't contain credentials)
#   # type: "postgres"                 # Database type: postgres, sqlite, duckdb
#   # host: "localhost"                # Database host
#   # port: 5432                       # Database port
#   # user: "crypto_user"              # Database username
#   # password: "secure_password"      # Database password
#   # database: "crypto_lake"          # Database name
#
#   # Connection pool settings (optional, SQLAlchemy only)
#   # pool_size: 5                     # Number of connections in pool
#   # max_overflow: 10                 # Maximum overflow connections
#   # pool_timeout: 30                 # Pool timeout in seconds

# Google Cloud Storage configuration (for Parquet data sync)
gcs:
  bucket_name: ""                     # GCS bucket name (e.g., "crypto-lake-data")

# Google Cloud Platform configuration (for Cloud Run deployment)
gcp:
  project_id: ""                      # GCP project ID
  region: ""                          # GCP region (e.g., "europe-west2")
